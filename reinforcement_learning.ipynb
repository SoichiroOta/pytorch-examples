{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"reinforcement_learning.ipynb","provenance":[],"authorship_tag":"ABX9TyM2J8crOjO3HNwnHb1XnB1M"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"fqZxMPPfxNzR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"da14d4a9-cf6d-43ed-f61e-c26bd15def1a","executionInfo":{"status":"ok","timestamp":1581632933489,"user_tz":-540,"elapsed":8215,"user":{"displayName":"太田宗一郎","photoUrl":"","userId":"09265366390614926947"}}},"source":["!pip install torch gym"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n","Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.15.6)\n","Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.2.2)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.17.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.10)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-A5A62KMq3yP","colab_type":"text"},"source":["# reinforce"]},{"cell_type":"code","metadata":{"id":"e-z9Rbljq3FU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":765},"outputId":"c5e49afc-c6ba-4679-afb3-676d69e14dfd","executionInfo":{"status":"ok","timestamp":1581633288636,"user_tz":-540,"elapsed":57124,"user":{"displayName":"太田宗一郎","photoUrl":"","userId":"09265366390614926947"}}},"source":["import gym\n","import numpy as np\n","from itertools import count\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.distributions import Categorical\n","\n","\n","gamma=0.99\n","seed=543\n","render=False\n","log_interval=10\n","\n","\n","env = gym.make('CartPole-v1')\n","env.seed(seed)\n","torch.manual_seed(seed)\n","\n","\n","class Policy(nn.Module):\n","    def __init__(self):\n","        super(Policy, self).__init__()\n","        self.affine1 = nn.Linear(4, 128)\n","        self.dropout = nn.Dropout(p=0.6)\n","        self.affine2 = nn.Linear(128, 2)\n","\n","        self.saved_log_probs = []\n","        self.rewards = []\n","\n","    def forward(self, x):\n","        x = self.affine1(x)\n","        x = self.dropout(x)\n","        x = F.relu(x)\n","        action_scores = self.affine2(x)\n","        return F.softmax(action_scores, dim=1)\n","\n","\n","policy = Policy()\n","optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n","eps = np.finfo(np.float32).eps.item()\n","\n","\n","def select_action(state):\n","    state = torch.from_numpy(state).float().unsqueeze(0)\n","    probs = policy(state)\n","    m = Categorical(probs)\n","    action = m.sample()\n","    policy.saved_log_probs.append(m.log_prob(action))\n","    return action.item()\n","\n","\n","def finish_episode():\n","    R = 0\n","    policy_loss = []\n","    returns = []\n","    for r in policy.rewards[::-1]:\n","        R = r + gamma * R\n","        returns.insert(0, R)\n","    returns = torch.tensor(returns)\n","    returns = (returns - returns.mean()) / (returns.std() + eps)\n","    for log_prob, R in zip(policy.saved_log_probs, returns):\n","        policy_loss.append(-log_prob * R)\n","    optimizer.zero_grad()\n","    policy_loss = torch.cat(policy_loss).sum()\n","    policy_loss.backward()\n","    optimizer.step()\n","    del policy.rewards[:]\n","    del policy.saved_log_probs[:]\n","\n","\n","def main():\n","    running_reward = 10\n","    for i_episode in count(1):\n","        state, ep_reward = env.reset(), 0\n","        for t in range(1, 10000):  # Don't infinite loop while learning\n","            action = select_action(state)\n","            state, reward, done, _ = env.step(action)\n","            if render:\n","                env.render()\n","            policy.rewards.append(reward)\n","            ep_reward += reward\n","            if done:\n","                break\n","\n","        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n","        finish_episode()\n","        if i_episode % log_interval == 0:\n","            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n","                  i_episode, ep_reward, running_reward))\n","        if running_reward > env.spec.reward_threshold:\n","            print(\"Solved! Running reward is now {} and \"\n","                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n","            break\n","\n","main()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"],"name":"stderr"},{"output_type":"stream","text":["Episode 10\tLast reward: 26.00\tAverage reward: 16.00\n","Episode 20\tLast reward: 16.00\tAverage reward: 14.85\n","Episode 30\tLast reward: 49.00\tAverage reward: 20.77\n","Episode 40\tLast reward: 45.00\tAverage reward: 27.37\n","Episode 50\tLast reward: 44.00\tAverage reward: 30.80\n","Episode 60\tLast reward: 111.00\tAverage reward: 42.69\n","Episode 70\tLast reward: 131.00\tAverage reward: 70.39\n","Episode 80\tLast reward: 87.00\tAverage reward: 76.68\n","Episode 90\tLast reward: 97.00\tAverage reward: 96.58\n","Episode 100\tLast reward: 156.00\tAverage reward: 97.38\n","Episode 110\tLast reward: 113.00\tAverage reward: 127.68\n","Episode 120\tLast reward: 194.00\tAverage reward: 176.93\n","Episode 130\tLast reward: 222.00\tAverage reward: 222.27\n","Episode 140\tLast reward: 111.00\tAverage reward: 233.86\n","Episode 150\tLast reward: 119.00\tAverage reward: 195.65\n","Episode 160\tLast reward: 89.00\tAverage reward: 145.78\n","Episode 170\tLast reward: 153.00\tAverage reward: 142.75\n","Episode 180\tLast reward: 358.00\tAverage reward: 273.09\n","Episode 190\tLast reward: 118.00\tAverage reward: 234.40\n","Episode 200\tLast reward: 46.00\tAverage reward: 173.96\n","Episode 210\tLast reward: 137.00\tAverage reward: 142.94\n","Episode 220\tLast reward: 298.00\tAverage reward: 169.32\n","Episode 230\tLast reward: 500.00\tAverage reward: 259.36\n","Episode 240\tLast reward: 500.00\tAverage reward: 327.65\n","Episode 250\tLast reward: 458.00\tAverage reward: 376.24\n","Episode 260\tLast reward: 500.00\tAverage reward: 371.39\n","Episode 270\tLast reward: 500.00\tAverage reward: 419.65\n","Episode 280\tLast reward: 430.00\tAverage reward: 433.79\n","Episode 290\tLast reward: 186.00\tAverage reward: 371.74\n","Episode 300\tLast reward: 192.00\tAverage reward: 308.58\n","Episode 310\tLast reward: 179.00\tAverage reward: 285.06\n","Episode 320\tLast reward: 175.00\tAverage reward: 237.67\n","Episode 330\tLast reward: 163.00\tAverage reward: 211.77\n","Episode 340\tLast reward: 290.00\tAverage reward: 213.61\n","Episode 350\tLast reward: 400.00\tAverage reward: 261.01\n","Episode 360\tLast reward: 277.00\tAverage reward: 253.11\n","Episode 370\tLast reward: 435.00\tAverage reward: 289.04\n","Episode 380\tLast reward: 500.00\tAverage reward: 373.69\n","Episode 390\tLast reward: 500.00\tAverage reward: 424.37\n","Episode 400\tLast reward: 500.00\tAverage reward: 454.72\n","Episode 410\tLast reward: 500.00\tAverage reward: 472.89\n","Solved! Running reward is now 475.53212844348985 and the last episode runs to 500 time steps!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VIpM2yCVsE1r","colab_type":"text"},"source":["# actor_critic"]},{"cell_type":"code","metadata":{"id":"hQfFwVkSsJoN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":680},"outputId":"c9f4fd69-80d5-4ea5-9a41-482c70d764ef","executionInfo":{"status":"ok","timestamp":1581633569742,"user_tz":-540,"elapsed":24085,"user":{"displayName":"太田宗一郎","photoUrl":"","userId":"09265366390614926947"}}},"source":["import gym\n","import numpy as np\n","from itertools import count\n","from collections import namedtuple\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.distributions import Categorical\n","\n","# Cart Pole\n","\n","gamma=0.99\n","seed=543\n","render=False\n","log_interval=10\n","\n","\n","env = gym.make('CartPole-v0')\n","env.seed(seed)\n","torch.manual_seed(seed)\n","\n","\n","SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n","\n","\n","class Policy(nn.Module):\n","    \"\"\"\n","    implements both actor and critic in one model\n","    \"\"\"\n","    def __init__(self):\n","        super(Policy, self).__init__()\n","        self.affine1 = nn.Linear(4, 128)\n","\n","        # actor's layer\n","        self.action_head = nn.Linear(128, 2)\n","\n","        # critic's layer\n","        self.value_head = nn.Linear(128, 1)\n","\n","        # action & reward buffer\n","        self.saved_actions = []\n","        self.rewards = []\n","\n","    def forward(self, x):\n","        \"\"\"\n","        forward of both actor and critic\n","        \"\"\"\n","        x = F.relu(self.affine1(x))\n","\n","        # actor: choses action to take from state s_t \n","        # by returning probability of each action\n","        action_prob = F.softmax(self.action_head(x), dim=-1)\n","\n","        # critic: evaluates being in the state s_t\n","        state_values = self.value_head(x)\n","\n","        # return values for both actor and critic as a tupel of 2 values:\n","        # 1. a list with the probability of each action over the action space\n","        # 2. the value from state s_t \n","        return action_prob, state_values\n","\n","\n","model = Policy()\n","optimizer = optim.Adam(model.parameters(), lr=3e-2)\n","eps = np.finfo(np.float32).eps.item()\n","\n","\n","def select_action(state):\n","    state = torch.from_numpy(state).float()\n","    probs, state_value = model(state)\n","\n","    # create a categorical distribution over the list of probabilities of actions\n","    m = Categorical(probs)\n","\n","    # and sample an action using the distribution\n","    action = m.sample()\n","\n","    # save to action buffer\n","    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n","\n","    # the action to take (left or right)\n","    return action.item()\n","\n","\n","def finish_episode():\n","    \"\"\"\n","    Training code. Calcultes actor and critic loss and performs backprop.\n","    \"\"\"\n","    R = 0\n","    saved_actions = model.saved_actions\n","    policy_losses = [] # list to save actor (policy) loss\n","    value_losses = [] # list to save critic (value) loss\n","    returns = [] # list to save the true values\n","\n","    # calculate the true value using rewards returned from the environment\n","    for r in model.rewards[::-1]:\n","        # calculate the discounted value\n","        R = r + gamma * R\n","        returns.insert(0, R)\n","\n","    returns = torch.tensor(returns)\n","    returns = (returns - returns.mean()) / (returns.std() + eps)\n","\n","    for (log_prob, value), R in zip(saved_actions, returns):\n","        advantage = R - value.item()\n","\n","        # calculate actor (policy) loss \n","        policy_losses.append(-log_prob * advantage)\n","\n","        # calculate critic (value) loss using L1 smooth loss\n","        value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))\n","\n","    # reset gradients\n","    optimizer.zero_grad()\n","\n","    # sum up all the values of policy_losses and value_losses\n","    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n","\n","    # perform backprop\n","    loss.backward()\n","    optimizer.step()\n","\n","    # reset rewards and action buffer\n","    del model.rewards[:]\n","    del model.saved_actions[:]\n","\n","\n","def main():\n","    running_reward = 10\n","\n","    # run inifinitely many episodes\n","    for i_episode in count(1):\n","\n","        # reset environment and episode reward\n","        state = env.reset()\n","        ep_reward = 0\n","\n","        # for each episode, only run 9999 steps so that we don't \n","        # infinite loop while learning\n","        for t in range(1, 10000):\n","\n","            # select action from policy\n","            action = select_action(state)\n","\n","            # take the action\n","            state, reward, done, _ = env.step(action)\n","\n","            if render:\n","                env.render()\n","\n","            model.rewards.append(reward)\n","            ep_reward += reward\n","            if done:\n","                break\n","\n","        # update cumulative reward\n","        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n","\n","        # perform backprop\n","        finish_episode()\n","\n","        # log results\n","        if i_episode % log_interval == 0:\n","            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n","                  i_episode, ep_reward, running_reward))\n","\n","        # check if we have \"solved\" the cart pole problem\n","        if running_reward > env.spec.reward_threshold:\n","            print(\"Solved! Running reward is now {} and \"\n","                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n","            break\n","\n","main()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"],"name":"stderr"},{"output_type":"stream","text":["Episode 10\tLast reward: 9.00\tAverage reward: 10.67\n","Episode 20\tLast reward: 10.00\tAverage reward: 14.35\n","Episode 30\tLast reward: 11.00\tAverage reward: 12.54\n","Episode 40\tLast reward: 10.00\tAverage reward: 11.31\n","Episode 50\tLast reward: 12.00\tAverage reward: 10.79\n","Episode 60\tLast reward: 9.00\tAverage reward: 10.43\n","Episode 70\tLast reward: 9.00\tAverage reward: 10.51\n","Episode 80\tLast reward: 12.00\tAverage reward: 11.04\n","Episode 90\tLast reward: 10.00\tAverage reward: 10.78\n","Episode 100\tLast reward: 32.00\tAverage reward: 12.34\n","Episode 110\tLast reward: 12.00\tAverage reward: 13.97\n","Episode 120\tLast reward: 26.00\tAverage reward: 19.78\n","Episode 130\tLast reward: 38.00\tAverage reward: 28.38\n","Episode 140\tLast reward: 65.00\tAverage reward: 48.45\n","Episode 150\tLast reward: 75.00\tAverage reward: 70.96\n","Episode 160\tLast reward: 84.00\tAverage reward: 69.84\n","Episode 170\tLast reward: 187.00\tAverage reward: 109.74\n","Episode 180\tLast reward: 26.00\tAverage reward: 81.50\n","Episode 190\tLast reward: 41.00\tAverage reward: 61.23\n","Episode 200\tLast reward: 57.00\tAverage reward: 55.29\n","Episode 210\tLast reward: 200.00\tAverage reward: 88.67\n","Episode 220\tLast reward: 28.00\tAverage reward: 114.71\n","Episode 230\tLast reward: 171.00\tAverage reward: 146.98\n","Episode 240\tLast reward: 200.00\tAverage reward: 167.15\n","Episode 250\tLast reward: 200.00\tAverage reward: 180.33\n","Episode 260\tLast reward: 200.00\tAverage reward: 188.22\n","Episode 270\tLast reward: 200.00\tAverage reward: 189.53\n","Episode 280\tLast reward: 70.00\tAverage reward: 168.98\n","Episode 290\tLast reward: 66.00\tAverage reward: 129.31\n","Episode 300\tLast reward: 66.00\tAverage reward: 112.47\n","Episode 310\tLast reward: 200.00\tAverage reward: 134.03\n","Episode 320\tLast reward: 200.00\tAverage reward: 153.57\n","Episode 330\tLast reward: 200.00\tAverage reward: 172.20\n","Episode 340\tLast reward: 200.00\tAverage reward: 179.79\n","Episode 350\tLast reward: 200.00\tAverage reward: 187.52\n","Episode 360\tLast reward: 200.00\tAverage reward: 192.53\n","Solved! Running reward is now 195.04375471448347 and the last episode runs to 200 time steps!\n"],"name":"stdout"}]}]}