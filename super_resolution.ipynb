{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"super_resolution.ipynb","provenance":[],"authorship_tag":"ABX9TyNfuAX0nEL6bU/KyYVtdGfg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"fqZxMPPfxNzR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b39ccc3b-940a-490e-a782-c79308790a67","executionInfo":{"status":"ok","timestamp":1581625986043,"user_tz":-540,"elapsed":3669,"user":{"displayName":"太田宗一郎","photoUrl":"","userId":"09265366390614926947"}}},"source":["!pip install torch"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HGJLy1_rOVsq","colab_type":"text"},"source":["# dataset"]},{"cell_type":"code","metadata":{"id":"f9rD0wxaOXE3","colab_type":"code","colab":{}},"source":["import torch.utils.data as data\n","\n","from os import listdir\n","from os.path import join\n","from PIL import Image\n","\n","\n","def is_image_file(filename):\n","    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\"])\n","\n","\n","def load_img(filepath):\n","    img = Image.open(filepath).convert('YCbCr')\n","    y, _, _ = img.split()\n","    return y\n","\n","\n","class DatasetFromFolder(data.Dataset):\n","    def __init__(self, image_dir, input_transform=None, target_transform=None):\n","        super(DatasetFromFolder, self).__init__()\n","        self.image_filenames = [join(image_dir, x) for x in listdir(image_dir) if is_image_file(x)]\n","\n","        self.input_transform = input_transform\n","        self.target_transform = target_transform\n","\n","    def __getitem__(self, index):\n","        input = load_img(self.image_filenames[index])\n","        target = input.copy()\n","        if self.input_transform:\n","            input = self.input_transform(input)\n","        if self.target_transform:\n","            target = self.target_transform(target)\n","\n","        return input, target\n","\n","    def __len__(self):\n","        return len(self.image_filenames)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eMGxo8jrOdey","colab_type":"text"},"source":["# data"]},{"cell_type":"code","metadata":{"id":"xQO9IhDmOfyn","colab_type":"code","colab":{}},"source":["from os.path import exists, join, basename\n","from os import makedirs, remove\n","from six.moves import urllib\n","import tarfile\n","from torchvision.transforms import Compose, CenterCrop, ToTensor, Resize\n","\n","\n","def download_bsd300(dest=\"dataset\"):\n","    output_image_dir = join(dest, \"BSDS300/images\")\n","\n","    if not exists(output_image_dir):\n","        makedirs(dest)\n","        url = \"http://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300-images.tgz\"\n","        print(\"downloading url \", url)\n","\n","        data = urllib.request.urlopen(url)\n","\n","        file_path = join(dest, basename(url))\n","        with open(file_path, 'wb') as f:\n","            f.write(data.read())\n","\n","        print(\"Extracting data\")\n","        with tarfile.open(file_path) as tar:\n","            for item in tar:\n","                tar.extract(item, dest)\n","\n","        remove(file_path)\n","\n","    return output_image_dir\n","\n","\n","def calculate_valid_crop_size(crop_size, upscale_factor):\n","    return crop_size - (crop_size % upscale_factor)\n","\n","\n","def input_transform(crop_size, upscale_factor):\n","    return Compose([\n","        CenterCrop(crop_size),\n","        Resize(crop_size // upscale_factor),\n","        ToTensor(),\n","    ])\n","\n","\n","def target_transform(crop_size):\n","    return Compose([\n","        CenterCrop(crop_size),\n","        ToTensor(),\n","    ])\n","\n","\n","def get_training_set(upscale_factor):\n","    root_dir = download_bsd300()\n","    train_dir = join(root_dir, \"train\")\n","    crop_size = calculate_valid_crop_size(256, upscale_factor)\n","\n","    return DatasetFromFolder(train_dir,\n","                             input_transform=input_transform(crop_size, upscale_factor),\n","                             target_transform=target_transform(crop_size))\n","\n","\n","def get_test_set(upscale_factor):\n","    root_dir = download_bsd300()\n","    test_dir = join(root_dir, \"test\")\n","    crop_size = calculate_valid_crop_size(256, upscale_factor)\n","\n","    return DatasetFromFolder(test_dir,\n","                             input_transform=input_transform(crop_size, upscale_factor),\n","                             target_transform=target_transform(crop_size))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rRH6fGa8Opz9","colab_type":"text"},"source":["# model"]},{"cell_type":"code","metadata":{"id":"ihx5Cl8AOv1A","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.init as init\n","\n","\n","class Net(nn.Module):\n","    def __init__(self, upscale_factor):\n","        super(Net, self).__init__()\n","\n","        self.relu = nn.ReLU()\n","        self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2))\n","        self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))\n","        self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1))\n","        self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))\n","        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n","\n","        self._initialize_weights()\n","\n","    def forward(self, x):\n","        x = self.relu(self.conv1(x))\n","        x = self.relu(self.conv2(x))\n","        x = self.relu(self.conv3(x))\n","        x = self.pixel_shuffle(self.conv4(x))\n","        return x\n","\n","    def _initialize_weights(self):\n","        init.orthogonal_(self.conv1.weight, init.calculate_gain('relu'))\n","        init.orthogonal_(self.conv2.weight, init.calculate_gain('relu'))\n","        init.orthogonal_(self.conv3.weight, init.calculate_gain('relu'))\n","        init.orthogonal_(self.conv4.weight)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3LRUhtwrOzCA","colab_type":"text"},"source":["# main"]},{"cell_type":"code","metadata":{"id":"rNWkMBl3O1fx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":377},"outputId":"3366e992-3f24-436a-95e4-8521484040a0","executionInfo":{"status":"ok","timestamp":1581626001693,"user_tz":-540,"elapsed":19247,"user":{"displayName":"太田宗一郎","photoUrl":"","userId":"09265366390614926947"}}},"source":["from __future__ import print_function\n","from math import log10\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","\n","\n","# Training settings\n","upscale_factor=3\n","batch_size=64\n","test_batch_size=10\n","n_epochs=2\n","lr=0.01\n","cuda=True\n","threads=4\n","seed=123\n","\n","\n","if cuda and not torch.cuda.is_available():\n","    raise Exception(\"No GPU found, please run without --cuda\")\n","\n","torch.manual_seed(seed)\n","\n","device = torch.device(\"cuda\" if cuda else \"cpu\")\n","\n","print('===> Loading datasets')\n","train_set = get_training_set(upscale_factor)\n","test_set = get_test_set(upscale_factor)\n","training_data_loader = DataLoader(dataset=train_set, num_workers=threads, batch_size=batch_size, shuffle=True)\n","testing_data_loader = DataLoader(dataset=test_set, num_workers=threads, batch_size=test_batch_size, shuffle=False)\n","\n","print('===> Building model')\n","model = Net(upscale_factor=upscale_factor).to(device)\n","criterion = nn.MSELoss()\n","\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","\n","def train(epoch):\n","    epoch_loss = 0\n","    for iteration, batch in enumerate(training_data_loader, 1):\n","        input, target = batch[0].to(device), batch[1].to(device)\n","\n","        optimizer.zero_grad()\n","        loss = criterion(model(input), target)\n","        epoch_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","\n","        print(\"===> Epoch[{}]({}/{}): Loss: {:.4f}\".format(epoch, iteration, len(training_data_loader), loss.item()))\n","\n","    print(\"===> Epoch {} Complete: Avg. Loss: {:.4f}\".format(epoch, epoch_loss / len(training_data_loader)))\n","\n","\n","def test():\n","    avg_psnr = 0\n","    with torch.no_grad():\n","        for batch in testing_data_loader:\n","            input, target = batch[0].to(device), batch[1].to(device)\n","\n","            prediction = model(input)\n","            mse = criterion(prediction, target)\n","            psnr = 10 * log10(1 / mse.item())\n","            avg_psnr += psnr\n","    print(\"===> Avg. PSNR: {:.4f} dB\".format(avg_psnr / len(testing_data_loader)))\n","\n","\n","def checkpoint(epoch):\n","    model_out_path = \"model_epoch_{}.pth\".format(epoch)\n","    torch.save(model, model_out_path)\n","    print(\"Checkpoint saved to {}\".format(model_out_path))\n","\n","for epoch in range(1, n_epochs + 1):\n","    train(epoch)\n","    test()\n","    checkpoint(epoch)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["===> Loading datasets\n","downloading url  http://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300-images.tgz\n","Extracting data\n","===> Building model\n","===> Epoch[1](1/4): Loss: 0.4304\n","===> Epoch[1](2/4): Loss: 29.2687\n","===> Epoch[1](3/4): Loss: 0.0979\n","===> Epoch[1](4/4): Loss: 0.2480\n","===> Epoch 1 Complete: Avg. Loss: 7.5113\n","===> Avg. PSNR: 6.6770 dB\n","Checkpoint saved to model_epoch_1.pth\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"},{"output_type":"stream","text":["===> Epoch[2](1/4): Loss: 0.2204\n","===> Epoch[2](2/4): Loss: 0.1196\n","===> Epoch[2](3/4): Loss: 0.0432\n","===> Epoch[2](4/4): Loss: 0.0374\n","===> Epoch 2 Complete: Avg. Loss: 0.1051\n","===> Avg. PSNR: 12.6057 dB\n","Checkpoint saved to model_epoch_2.pth\n"],"name":"stdout"}]}]}