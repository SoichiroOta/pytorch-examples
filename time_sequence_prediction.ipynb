{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"time_sequence_prediction.ipynb","provenance":[],"authorship_tag":"ABX9TyNL9yjMB8bV1vlmX9gHeR69"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"fqZxMPPfxNzR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"998b0ea8-9eea-4703-80a5-1fac1e727647","executionInfo":{"status":"ok","timestamp":1581555309824,"user_tz":-540,"elapsed":3860,"user":{"displayName":"太田宗一郎","photoUrl":"","userId":"09265366390614926947"}}},"source":["!pip install torch"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LCuKy4lnCqOQ","colab_type":"code","colab":{}},"source":["import numpy as np\n","import torch\n","\n","np.random.seed(2)\n","\n","T = 20\n","L = 1000\n","N = 100\n","\n","x = np.empty((N, L), 'int64')\n","x[:] = np.array(range(L)) + np.random.randint(-4 * T, 4 * T, N).reshape(N, 1)\n","data = np.sin(x / 1.0 / T).astype('float64')\n","torch.save(data, open('traindata.pt', 'wb'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d7oxNuVzCx2k","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"b497db2e-6518-4786-8c46-a763bec062ac","executionInfo":{"status":"ok","timestamp":1581555873495,"user_tz":-540,"elapsed":490184,"user":{"displayName":"太田宗一郎","photoUrl":"","userId":"09265366390614926947"}}},"source":["from __future__ import print_function\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import matplotlib\n","matplotlib.use('Agg')\n","import matplotlib.pyplot as plt\n","\n","class Sequence(nn.Module):\n","    def __init__(self):\n","        super(Sequence, self).__init__()\n","        self.lstm1 = nn.LSTMCell(1, 51)\n","        self.lstm2 = nn.LSTMCell(51, 51)\n","        self.linear = nn.Linear(51, 1)\n","\n","    def forward(self, input, future = 0):\n","        outputs = []\n","        h_t = torch.zeros(input.size(0), 51, dtype=torch.double)\n","        c_t = torch.zeros(input.size(0), 51, dtype=torch.double)\n","        h_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)\n","        c_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)\n","\n","        for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):\n","            h_t, c_t = self.lstm1(input_t, (h_t, c_t))\n","            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n","            output = self.linear(h_t2)\n","            outputs += [output]\n","        for i in range(future):# if we should predict the future\n","            h_t, c_t = self.lstm1(output, (h_t, c_t))\n","            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n","            output = self.linear(h_t2)\n","            outputs += [output]\n","        outputs = torch.stack(outputs, 1).squeeze(2)\n","        return outputs\n","\n","\n","def main():\n","    # set random seed to 0\n","    np.random.seed(0)\n","    torch.manual_seed(0)\n","    # load data and make training set\n","    data = torch.load('traindata.pt')\n","    input = torch.from_numpy(data[3:, :-1])\n","    target = torch.from_numpy(data[3:, 1:])\n","    test_input = torch.from_numpy(data[:3, :-1])\n","    test_target = torch.from_numpy(data[:3, 1:])\n","    # build the model\n","    seq = Sequence()\n","    seq.double()\n","    criterion = nn.MSELoss()\n","    # use LBFGS as optimizer since we can load the whole data to train\n","    optimizer = optim.LBFGS(seq.parameters(), lr=0.8)\n","    #begin to train\n","    for i in range(15):\n","        print('STEP: ', i)\n","        def closure():\n","            optimizer.zero_grad()\n","            out = seq(input)\n","            loss = criterion(out, target)\n","            print('loss:', loss.item())\n","            loss.backward()\n","            return loss\n","        optimizer.step(closure)\n","        # begin to predict, no need to track gradient here\n","        with torch.no_grad():\n","            future = 1000\n","            pred = seq(test_input, future=future)\n","            loss = criterion(pred[:, :-future], test_target)\n","            print('test loss:', loss.item())\n","            y = pred.detach().numpy()\n","        # draw the result\n","        plt.figure(figsize=(30,10))\n","        plt.title('Predict future values for time sequences\\n(Dashlines are predicted values)', fontsize=30)\n","        plt.xlabel('x', fontsize=20)\n","        plt.ylabel('y', fontsize=20)\n","        plt.xticks(fontsize=20)\n","        plt.yticks(fontsize=20)\n","        def draw(yi, color):\n","            plt.plot(np.arange(input.size(1)), yi[:input.size(1)], color, linewidth = 2.0)\n","            plt.plot(np.arange(input.size(1), input.size(1) + future), yi[input.size(1):], color + ':', linewidth = 2.0)\n","        draw(y[0], 'r')\n","        draw(y[1], 'g')\n","        draw(y[2], 'b')\n","        plt.savefig('predict%d.pdf'%i)\n","        plt.close()\n","main()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["STEP:  0\n","loss: 0.502373812247558\n","loss: 0.4985663937943566\n","loss: 0.4790119606115284\n","loss: 0.44633490214842253\n","loss: 0.35406310257492923\n","loss: 0.20507016617681412\n","loss: 1.3960531561167624\n","loss: 0.032494411484718155\n","loss: 0.02993487583960432\n","loss: 0.02832682101153415\n","loss: 0.026830612218823123\n","loss: 0.02377120198998976\n","loss: 0.018901413504545748\n","loss: 0.01064681823320575\n","loss: 0.008725752090268499\n","loss: 0.00787218128777726\n","loss: 0.0054778427495943815\n","loss: 0.00405193356406391\n","loss: 0.0027296227011591205\n","loss: 0.0015402652769807234\n","test loss: 0.0013000876156955348\n","STEP:  1\n","loss: 0.001279764616782664\n","loss: 0.0011690554954716906\n","loss: 0.001149891601372753\n","loss: 0.00112882452912809\n","loss: 0.0010630561341025105\n","loss: 0.000956400644328242\n","loss: 0.0008210827589983624\n","loss: 0.0007670796029145448\n","loss: 0.0007294716423578014\n","loss: 0.0007246558169811339\n","loss: 0.000720621139364854\n","loss: 0.0007126686053106413\n","loss: 0.0006961310498769056\n","loss: 0.0006641101811541216\n","loss: 0.0006101210702275329\n","loss: 0.0005285228387101468\n","loss: 0.0004127250710281541\n","loss: 0.0003302157945217243\n","loss: 0.0003121712149398955\n","loss: 0.00032352192206046976\n","test loss: 0.00017005112895102716\n","STEP:  2\n","loss: 0.00030530789272772047\n","loss: 0.0003041496583644488\n","loss: 0.0003035129747820695\n","loss: 0.00030276531287183947\n","loss: 0.00030103213900653405\n","loss: 0.0002974408442645935\n","loss: 0.00029076089116465134\n","loss: 0.00028094788597949016\n","loss: 0.0002682227137291152\n","loss: 0.00025221919695802945\n","loss: 0.00023920222327947314\n","loss: 0.00022590525893533285\n","loss: 0.00022719613064847377\n","loss: 0.00020802021407370332\n","loss: 0.00020050239660521105\n","loss: 0.00019149492309837132\n","loss: 0.00018777715169048103\n","loss: 0.00018122790438083037\n","loss: 0.00017671387790268916\n","loss: 0.00016960355098207244\n","test loss: 6.615079325038828e-05\n","STEP:  3\n","loss: 0.0001600138819938286\n","loss: 0.00015376838500039973\n","loss: 0.0001515606043968558\n","loss: 0.00015186142351500794\n","loss: 0.00015115942793504\n","loss: 0.00015102884213333387\n","loss: 0.00015083292548274304\n","loss: 0.000150706321922209\n","loss: 0.00015045822237195078\n","loss: 0.00015005142516714517\n","loss: 0.00014972513291178574\n","loss: 0.0001490297011570734\n","loss: 0.0001479109192078194\n","loss: 0.000145924747069613\n","loss: 0.0001423960868987725\n","loss: 0.0001377504545587692\n","loss: 0.0001234136004281233\n","loss: 0.00010522420859041967\n","loss: 0.00012598326432512665\n","loss: 0.0001058410007935883\n","test loss: 4.079810097085338e-05\n","STEP:  4\n","loss: 9.140770553176714e-05\n","loss: 8.8523624845152e-05\n","loss: 8.404066702022784e-05\n","loss: 7.90864491458718e-05\n","loss: 7.640386916479052e-05\n","loss: 7.338808656498467e-05\n","loss: 7.153853831075943e-05\n","loss: 7.106763578240051e-05\n","loss: 7.068615191012888e-05\n","loss: 7.055590730591918e-05\n","loss: 7.051953971874027e-05\n","loss: 7.048190468272856e-05\n","loss: 7.016759636647337e-05\n","loss: 6.959172658433367e-05\n","loss: 6.871311056014111e-05\n","loss: 6.760285216165817e-05\n","loss: 6.273037111499346e-05\n","loss: 5.7851856425783026e-05\n","loss: 0.00019957009555918767\n","loss: 5.609367597209388e-05\n","test loss: 7.003350834337507e-05\n","STEP:  5\n","loss: 7.66649025206451e-05\n","loss: 4.296445569016575e-05\n","loss: 3.679822462576974e-05\n","loss: 3.453205437132892e-05\n","loss: 2.9614649106326226e-05\n","loss: 2.844967455185457e-05\n","loss: 2.714908637300283e-05\n","loss: 2.5130994199590775e-05\n","loss: 2.4123115790596945e-05\n","loss: 2.3902182787479694e-05\n","loss: 2.3767411724673423e-05\n","loss: 2.374813491318665e-05\n","loss: 2.3703882142060442e-05\n","loss: 2.3662998104299546e-05\n","loss: 2.3580773971158452e-05\n","loss: 2.3374534768554045e-05\n","loss: 2.3087214001720696e-05\n","loss: 2.289039574434206e-05\n","loss: 2.2630472326022568e-05\n","loss: 2.20055119710054e-05\n","test loss: 2.518930500779705e-05\n","STEP:  6\n","loss: 2.0534598202724034e-05\n","loss: 1.8993649309093156e-05\n","loss: 1.793562631633175e-05\n","loss: 1.7232028909721643e-05\n","loss: 1.6924673394088787e-05\n","loss: 1.6833810036074454e-05\n","loss: 1.681541205824884e-05\n","loss: 1.6803870136315697e-05\n","loss: 1.6774988600819506e-05\n","loss: 1.6595227728532703e-05\n","loss: 1.6244260745424966e-05\n","loss: 1.5540425709186395e-05\n","loss: 1.4865012763109387e-05\n","loss: 1.4548720955845535e-05\n","loss: 1.4204195060363796e-05\n","loss: 1.4041676904454867e-05\n","loss: 1.382711517898867e-05\n","loss: 1.3538966027773415e-05\n","loss: 1.323249858269575e-05\n","loss: 1.3051562021718072e-05\n","test loss: 1.430243095965558e-05\n","STEP:  7\n","loss: 1.2888775559721012e-05\n","loss: 1.2813266229944483e-05\n","loss: 1.2761023626058231e-05\n","loss: 1.2675968822096544e-05\n","loss: 1.2548578250481964e-05\n","loss: 1.233253006483662e-05\n","loss: 1.2032434666587441e-05\n","loss: 1.1557540594698107e-05\n","loss: 1.0787811547275101e-05\n","loss: 1.3718557873160487e-05\n","loss: 9.998189363859262e-06\n","loss: 9.813901605101632e-06\n","loss: 9.259588624734208e-06\n","loss: 8.897361573218263e-06\n","loss: 8.509943187595106e-06\n","loss: 8.246029555789069e-06\n","loss: 8.165260117405314e-06\n","loss: 7.961256404644677e-06\n","loss: 7.844241667784901e-06\n","loss: 7.5714985433931e-06\n","test loss: 8.915540188857136e-06\n","STEP:  8\n","loss: 7.2351999049657085e-06\n","loss: 7.012088995465153e-06\n","loss: 6.981378928067383e-06\n","loss: 6.9004734667719896e-06\n","loss: 6.877598271927991e-06\n","loss: 6.8640086785199505e-06\n","loss: 6.85571819272458e-06\n","loss: 6.851717288702821e-06\n","loss: 6.847019346755626e-06\n","loss: 6.841138852674547e-06\n","loss: 6.834347630580622e-06\n","loss: 6.829190402422816e-06\n","loss: 6.8239271643900035e-06\n","loss: 6.819281820764999e-06\n","loss: 6.8145584110913855e-06\n","loss: 6.804658775768789e-06\n","loss: 6.785302057016542e-06\n","loss: 6.749153609022714e-06\n","loss: 6.674473991214601e-06\n","loss: 6.529923004665445e-06\n","test loss: 8.56154528772954e-06\n","STEP:  9\n","loss: 6.2964890059181e-06\n","loss: 6.027143707346752e-06\n","loss: 7.265865929140881e-06\n","loss: 5.910416008932747e-06\n","loss: 7.470379431503674e-06\n","loss: 5.973486559721328e-06\n","loss: 5.8065336224398824e-06\n","loss: 5.532251328266124e-06\n","loss: 5.480781444487052e-06\n","loss: 5.342050633490313e-06\n","loss: 5.2634995844682295e-06\n","loss: 5.193014718161045e-06\n","loss: 5.0928340894608275e-06\n","loss: 4.92815353868193e-06\n","loss: 4.865678462358072e-06\n","loss: 4.823627663116772e-06\n","loss: 4.810001194279997e-06\n","loss: 4.8060205077532895e-06\n","loss: 4.8051749786758804e-06\n","test loss: 7.99303583752279e-06\n","STEP:  10\n","loss: 4.8051749786758804e-06\n","loss: 4.803644210413042e-06\n","loss: 4.799981148465308e-06\n","loss: 4.796955932642759e-06\n","loss: 4.795050562847928e-06\n","loss: 4.793938130224186e-06\n","loss: 4.793105324620166e-06\n","test loss: 7.982745234195877e-06\n","STEP:  11\n","loss: 4.793105324620166e-06\n","loss: 4.791480833581121e-06\n","loss: 4.788189167293501e-06\n","loss: 4.7831476807199985e-06\n","loss: 4.777275897230671e-06\n","loss: 4.772684797545606e-06\n","loss: 4.769652863685107e-06\n","loss: 4.7658528331326325e-06\n","loss: 4.758617760760957e-06\n","loss: 4.74414633088257e-06\n","loss: 4.715616784673259e-06\n","loss: 4.6644014154056436e-06\n","loss: 4.592956861949275e-06\n","loss: 4.6728077745423e-06\n","loss: 4.480749603291079e-06\n","loss: 4.450949108977322e-06\n","loss: 4.42606992397347e-06\n","loss: 4.412831359878359e-06\n","loss: 4.405893741443204e-06\n","loss: 4.404458572668337e-06\n","test loss: 7.282763971258479e-06\n","STEP:  12\n","loss: 4.403664367453628e-06\n","loss: 4.40294679158283e-06\n","test loss: 7.2767025952669665e-06\n","STEP:  13\n","loss: 4.40294679158283e-06\n","test loss: 7.2767025952669665e-06\n","STEP:  14\n","loss: 4.40294679158283e-06\n","test loss: 7.2767025952669665e-06\n"],"name":"stdout"}]}]}